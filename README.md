 # âœ¨ ELoRA: Efficient LoRA and KV Cache Management for Multi-LoRA LLMs âœ¨

  ## ğŸ” Overview

  ELoRA is a supercharged ğŸš€ inference system that transforms Multi-LoRA LLM serving through smart management of LoRA adapters and KV caches based on their usage patterns. By cleverly tracking these dependencies in a unified caching framework, ELoRA dramatically boosts inference efficiency and slashes latency in real-world production environments!

  ## ğŸ† Breakthrough Performance

  <div align="center">
  <b>ğŸ¯ ELoRA delivers mind-blowing performance improvements over existing systems:</b>
  </div>

  - âš¡ **59.7% reduction** in Time-To-First-Token (TTFT)
  - ğŸ”¥ **33.2% reduction** in Time-Per-Output-Token (TPOT)
  - ğŸš€ **1.9Ã— higher** peak throughput

  ## ğŸ’¡ Key Innovations

  - **ğŸ§  Dependency-Aware Cache Manager**: Intelligently eliminates invalid KV caches by tracking usage dependencies between LoRAs and KV caches
  - **âš™ï¸ Performance-Driven Cache Swapper**: Makes brilliant swap decisions based on a unified cost model that precisely calculates benefits to inference performance
  - **ğŸŠâ€â™‚ï¸ Unified Caching Pool**: Dynamically juggles memory resources between LoRAs and KV caches based on what your workload actually needs in real-time

  ## ğŸŒ Broad Compatibility

  - **ğŸ“š Models**: Works seamlessly with Llama2-7B/13B/34B and other decoder-only transformer architectures
  - **ğŸ’» Hardware**: Runs efficiently across various accelerators (NPUs, GPUs)
  - **ğŸ› ï¸ Applications**: Perfect for chatbots, multi-language translation, personal assistants, and all your Multi-LoRA needs!

## ğŸ› ï¸ Quick Start Guide

### ğŸ³ Setting Up Your Environment

Get up and running in minutes with our Docker-based setup:

```bash
# Launch container with GPU support
docker run --name yourname -d --gpus all -it --ipc=host \
  -v /your/path/here:/root -p 12345:22 nvcr.io/nvidia/pytorch:23.10-py3

# Clone the ELoRA repository
git clone https://github.com/GiggleWang/ELoRA.git

# Navigate to project directory
cd ELoRA

# Enable multi-LoRA capability
export VLLM_INSTALL_PUNICA_KERNELS=1

# Install ELoRA
pip install -e .
```

### ğŸš€ Running ELoRA

Fire up the API server and start making inference requests:

```bash
# Launch the API server
python /root/ELoRA/vllm/entrypoints/openai/api_server.py \
    --model your_model/ \
    --lora-module your_lora

# Make your first request!
curl -X POST http://0.0.0.0:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "your_model_and_lora",
    "prompt": "tell me something about ELoRA",
    "elora_id": your_elora_id,
    "max_tokens": 100,
    "temperature": 0.0,
    "stream": false,
    "ignore_eos": true
  }'
```

Now you're ready to experience the lightning-fast âš¡ performance of ELoRA with your own models and LoRA adapters!
